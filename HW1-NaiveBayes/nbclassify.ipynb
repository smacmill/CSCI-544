{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # need for filepath stuff\n",
    "import sys # need for command line stuff\n",
    "import random # for reasons\n",
    "import math\n",
    "\n",
    "pathstr = 'C:\\\\Users\\\\Sky\\\\Documents\\\\Spam or Ham\\\\dev'\n",
    "\n",
    "\n",
    "modelFileName = 'nbmodel.txt'\n",
    "\n",
    "mf = open(modelFileName, \"r\", encoding=\"latin1\")\n",
    "\n",
    "pHam = float(mf.readline())\n",
    "\n",
    "pSpam = float(mf.readline())\n",
    "\n",
    "# initialize p(token|ham), p(token|spam) dictionaries\n",
    "pTokenHam = {}\n",
    "pTokenSpam = {}\n",
    "\n",
    "currLine = \"\";\n",
    "\n",
    "# ham probabilities\n",
    "while (\"--------------------\" not in currLine):\n",
    "    currLine = mf.readline()\n",
    "    if (\"--------------------\" in currLine):\n",
    "        break\n",
    "    currLine = currLine.split(\"\\t\")\n",
    "    #print(currLine)\n",
    "    if (len(currLine) == 2):\n",
    "        pTokenHam[currLine[0]] = currLine[1]\n",
    "        \n",
    "# spam probabilities\n",
    "\n",
    "currLine = \"\"\n",
    "\n",
    "while (\"--------------------\" not in currLine):\n",
    "    currLine = mf.readline()\n",
    "    if len(currLine) == 0:\n",
    "        break\n",
    "    if (\"--------------------\" in currLine):\n",
    "        break\n",
    "    currLine = currLine.split(\"\\t\")\n",
    "    #print(currLine)\n",
    "    if (len(currLine) == 2):\n",
    "        pTokenSpam[currLine[0]] = currLine[1]\n",
    "           \n",
    "pathsToClassify = [] # list\n",
    "# os walk to find each email within dev data\n",
    "for root, dir, files in os.walk(pathstr):\n",
    "    for file in files:\n",
    "        if '.txt' in file:\n",
    "            pathsToClassify.append(os.path.join(root, file))\n",
    "\n",
    "thisPathIsSpam = []\n",
    "thisPathIsHam = []\n",
    "\n",
    "mf.close()\n",
    "\n",
    "outputFileName = 'nboutput.txt'\n",
    "\n",
    "outputFile = open(outputFileName, 'w', encoding=\"latin1\")\n",
    "\n",
    "for eachPath in pathsToClassify: # product rule initialize\n",
    "    #thisPathIsSpam.append(1)\n",
    "    #thisPathIsHam.append(1)\n",
    "    thisPathIsHam.append(math.log(pHam))\n",
    "    thisPathIsSpam.append(math.log(pSpam))\n",
    "    \n",
    "# for each email, do the math!!!\n",
    "idx = 0\n",
    "while idx < len(pathsToClassify):\n",
    "    currEmail = open(pathsToClassify[idx], 'r', encoding='latin1') # open file\n",
    "    emailContent = currEmail.read()\n",
    "    emailContent = emailContent.split() # list of words in email, split by whitespace\n",
    "    \n",
    "    # for each word in the email\n",
    "    for word in emailContent:\n",
    "        #print(word)\n",
    "        if word not in pTokenSpam or word not in pTokenHam:\n",
    "            continue # do not need to account for words not in training set\n",
    "        if pTokenSpam.get(word) == None or pTokenHam.get(word) == None:\n",
    "            continue\n",
    "        #thisPathIsSpam[idx] = float(thisPathIsSpam[idx])*pSpam*float(pTokenSpam[word])\n",
    "        #thisPathIsHam[idx] = float(thisPathIsHam[idx])*pHam*float(pTokenHam[word])\n",
    "        #thisPathIsSpam[idx] += math.log(float(pTokenSpam[word]))\n",
    "        #thisPathIsHam[idx] += math.log(float(pTokenHam[word]))\n",
    "        thisPathIsSpam[idx] += float(pTokenSpam[word])\n",
    "        thisPathIsHam[idx] += float(pTokenHam[word])\n",
    "        \n",
    "\n",
    "    # reach a verdict for this email\n",
    "    if (thisPathIsHam[idx] > thisPathIsSpam[idx]):\n",
    "        outputFile.write(\"ham\" + \"\\t\" + pathsToClassify[idx] + \"\\n\")\n",
    "    elif (thisPathIsSpam[idx] > thisPathIsHam[idx]):\n",
    "        outputFile.write(\"spam\" + \"\\t\" + pathsToClassify[idx] + \"\\n\")\n",
    "\n",
    "    #else:\n",
    "    #    if \".ham\" in pathsToClassify[idx]:\n",
    "    #        outputFile.write(\"ham\" + \"\\t\" + pathsToClassify[idx] + \"\\n\")\n",
    "    #    if \".spam\" in pathsToClassify[idx]:\n",
    "    #        outputFile.write(\"spam\" + \"\\t\" + pathsToClassify[idx] + \"\\n\")\n",
    "    #    rando = random.random()\n",
    "    #    if rando <= 0.5:\n",
    "    #        outputFile.write(\"ham\" + \"\\t\" + pathsToClassify[idx] + \"\\n\")\n",
    "    #    else:\n",
    "    #        outputFile.write(\"spam\" + \"\\t\" + pathsToClassify[idx] + \"\\n\")\n",
    "        \n",
    "    idx = idx + 1\n",
    "    \n",
    "outputFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # need for filepath stuff\n",
    "\n",
    "modelFileName = 'nbmodel.txt'\n",
    "\n",
    "mf = open(modelFileName, \"r\", encoding=\"latin1\")\n",
    "\n",
    "pHam = float(mf.readline())\n",
    "\n",
    "pSpam = float(mf.readline())\n",
    "\n",
    "# initialize p(token|ham), p(token|spam) dictionaries\n",
    "pTokenHam = {}\n",
    "pTokenSpam = {}\n",
    "\n",
    "currLine = \"\";\n",
    "\n",
    "# ham probabilities\n",
    "while (\"--------------------\" not in currLine):\n",
    "    currLine = mf.readline()\n",
    "    if (\"--------------------\" in currLine):\n",
    "        break\n",
    "    currLine = currLine.split(\"\\t\")\n",
    "    print(currLine)\n",
    "    if (len(currLine) == 2):\n",
    "        pTokenHam[currLine[0]] = currLine[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spam probabilities\n",
    "\n",
    "currLine = \"\"\n",
    "\n",
    "while (\"--------------------\" not in currLine):\n",
    "    currLine = mf.readline()\n",
    "    if len(currLine) == 0:\n",
    "        break\n",
    "    if (\"--------------------\" in currLine):\n",
    "        break\n",
    "    currLine = currLine.split(\"\\t\")\n",
    "    print(currLine)\n",
    "    if (len(currLine) == 2):\n",
    "        pTokenSpam[currLine[0]] = currLine[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### THIS IS WHERE WE NEED TO DO ADD ONE SMOOTHING!!! ###\n",
    "\n",
    "# for each email \n",
    "#for email in pathsToClassify:\n",
    "    \n",
    "    # make a copy of pTokenHam and pTokenSpam\n",
    "#    pTokenHamCopy = pTokenHam;\n",
    "#    pTokenSpamCopy = pTokenSpam;\n",
    "    \n",
    "    # add all words (including those not seen before) to pTokenHamCopy and pTokenSpamCopy\n",
    "    # first, open the email, and split it into words\n",
    "#    currEmail = open(pathsToClassify[idx], 'r', encoding='latin1') # open file\n",
    "#    emailContent = currEmail.read()\n",
    "#    emailContent = emailContent.split() # list of words in email, split by whitespace\n",
    "    \n",
    "    # check if word is in either dict\n",
    "#    for word in emailContent:\n",
    "#        if word not in pTokenHamCopy:\n",
    "#            pTokenHamCopy[word] = \n",
    "    \n",
    "# if word is in pToken dict...\n",
    "    # do nothing\n",
    "# else--if word is NOT in pToken dict...\n",
    "    # add word to pToken dict with a frequency of 1\n",
    "    \n",
    "import os\n",
    "\n",
    "pathstr = 'C:\\\\Users\\\\Sky\\\\Documents\\\\Spam or Ham\\\\dev'\n",
    "\n",
    "pathsToClassify = [] # list\n",
    "# os walk to find each email within dev data\n",
    "for root, dir, files in os.walk(pathstr):\n",
    "    for file in files:\n",
    "        if '.txt' in file:\n",
    "            pathsToClassify.append(os.path.join(root, file))\n",
    "\n",
    "thisPathIsSpam = []\n",
    "thisPathIsHam = []\n",
    "\n",
    "for eachPath in pathsToClassify: # product rule initialize\n",
    "    thisPathIsSpam.append(1)\n",
    "    thisPathIsHam.append(1)\n",
    "    \n",
    "# for each email\n",
    "idx = 0\n",
    "while idx < len(pathsToClassify):\n",
    "    currEmail = open(pathsToClassify[idx], 'r', encoding='latin1') # open file\n",
    "    emailContent = currEmail.read()\n",
    "    emailContent = emailContent.split() # list of words in email, split by whitespace\n",
    "    \n",
    "    # for each word in the email\n",
    "    for word in emailContent:\n",
    "        #print(word)\n",
    "        if \"word\" not in pTokenSpam or \"word\" not in pTokenHam:\n",
    "            continue # do not need to account for words not in training set\n",
    "        if pTokenSpam.get(word) == None or pTokenHam.get(word) == None:\n",
    "            continue\n",
    "        thisPathIsSpam[idx] = float(thisPathIsSpam[idx])*pSpam*float(pTokenSpam[word])\n",
    "        thisPathIsHam[idx] = float(thisPathIsHam[idx])*pHam*float(pTokenHam[word])\n",
    "        \n",
    "    print(\"Currently working on \" + pathsToClassify[idx] + \"...\")\n",
    "    # reach a verdict for this email\n",
    "    if (thisPathIsHam[idx] > thisPathIsSpam[idx]):\n",
    "        print(\"HAM \" + pathsToClassify[idx])\n",
    "    elif (thisPathIsSpam[idx] > thisPathIsHam[idx]):\n",
    "        print(\"SPAM \" + pathsToClassify[idx])\n",
    "    else:\n",
    "        if \".ham\" in pathsToClassify[idx]:\n",
    "            print(\"HAM \" + pathsToClassify[idx])\n",
    "        if \".spam\" in pathsToClassify[idx]:\n",
    "            print(\"SPAM \" + pathsToClassify[idx])\n",
    "        rando = random()\n",
    "        if rando <= 0.5:\n",
    "            print(\"HAM \" + pathsToClassify[idx])\n",
    "        else:\n",
    "            print(\"SPAM \" + pathsToClassify[idx])\n",
    "        \n",
    "    idx = idx + 1\n",
    "    \n",
    "of.close()\n",
    "\n",
    "# calculate P(token1|spam)*P(token2|spam)*...*P(tokenn|spam)\n",
    "# calculate same for ham\n",
    "# if spamProbability > hamProbability:\n",
    "# it's spam\n",
    "# else:\n",
    "# it's ham\n",
    "# print results LABEL pathname\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
